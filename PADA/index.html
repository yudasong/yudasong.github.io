<!DOCTYPE html>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Policy Adaptation</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="../css/bootstrap.min.css">
  <link rel="stylesheet" href="../css/bootstrap-theme.min.css">
  <!-- Google fonts -->
  <link href="../css/google-fonts.css" rel="stylesheet" type="text/css">
  <!-- Google Analytics -->
  <link rel="stylesheet" type="text/css" href="style.css">

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-168693378-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-168693378-1');
  </script>


</head>
<body onload="page_loaded()">

<div id="header">
  <h1><b>Provably Efficient Model-based Policy Adaptation</b></h1>
  <center>
      <br>
      <font size="5"><a href="https://yudasong.github.io">Yuda Song</a>  <sup>1</sup>, <a href="https://aditimavalankar.github.io/">Aditi Mavalankar </a> <sup>1</sup>,
       <a href="https://wensun.github.io/">Wen Sun </a> <sup>2,3</sup>, <a href="http://scungao.github.io/">Sicun Gao </a> <sup>1</sup></font>
      <br>
      <nobr>1 University of California, San Diego</nobr> <br>
      <nobr>2 Cornell University</nobr> <br>
      <nobr>3 Microsoft Research</nobr> <br>
      <br>
      <font size="4">  <a href="https://arxiv.org/pdf/2006.08051.pdf">[Paper]</a> &nbsp&nbsp&nbsp&nbsp<a href="https://github.com/yudasong/policy_adapt">[Code]</a> &nbsp&nbsp&nbsp&nbsp  <a href="https://icml.cc/virtual/2020/poster/6588">[Talk]</a>  &nbsp&nbsp&nbsp&nbsp<a href="slides.pdf">[Slides]</a> </font>
      <br>
  </center>
  <div style="clear:both;"></div>
</div>

<div class="sechighlight">
<div class="container sec">
  <center><h2>Abstract</h2></center>
  The high sample complexity of reinforcement learning challenges its use in practice. A promising approach is to quickly adapt pre-trained policies to new environments. Existing methods for this policy adaptation problem typically rely on domain randomization and meta-learning, by sampling from some distribution of target environments during pre-training, and thus face difficulty on out-of-distribution target environments. We propose new model-based mechanisms that are able to make online adaptation in unseen target environments, by combining ideas from no-regret online learning and adaptive control. We prove that the approach learns policies in the target environment that can quickly recover trajectories from the source environment, and establish the rate of convergence in general settings. We demonstrate the benefits of our approach for policy adaptation in a diverse set of continuous control tasks, achieving the performance of state-of-the-art methods with much lower sample complexity.
</div>
</div>

<div class="container sec">
    <h2>Why adaptation? </h2>
    <div class='row'>
      <div style="margin-top: 20px" class='col-xs-3'>
    It is easy to train a good policy in an environment that we have control on. But what if our ultimate goal is to have a good policy that
    works on another environment that we have very limited knowledge of? In most of the time, directly deploying the policy in hand to an unknown
    environment will result in failure - reinforcement learning algorithms are not that robust. This is why we need an extra step to adapt the policy
    to the other environment.
      </div>
      <div class='col-xs-9'>
        <img style="width: 100%" src='img/intro.png'>
      </div>
    </div>
</div>
<div class="container sec">
    <h2>Overall Algorithm: </h2>
    <div class='row'>
      <div class='col-xs-8'>
        <center>
      <img style="width: 80%" src='img/algo.png'>
    </center>
      </div>
      <div style="margin-top: 20px" class='col-xs-4'>
    One way to ensure the performance of the adapted policy is to make it recover the source policy's trajectories. To do this, we
    realize that it suffices to learn the transitional dynamics of the environment that we are not familiar with. We propose a method
    that utilize the Data Aggregation scheme and we prove the rate of convergence of our algorithm. In practice, we train a neural network
    that predict the deviation between the two environments and use cross entropy method to find the action that minimizes the deviation.
      </div>

</div>







<div class="container sec">
    <h2>Example Results: </h2>


    <div class='row'>
        <div class='col-xs-4'>
        <h3>Original policy </h3>
        </div>
        <div class='col-xs-4'>
            <h3>Directly deploy the original policy </h3>
            </div>
        <div class='col-xs-4'>
            <h3>Adapted policy </h3>
        </div>
    </div>

    <div class='row'>
      <div class='col-xs-4'>
<!--                    <iframe width="560" height="315" src="img/3-link-balancing.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen> </iframe>-->
      <div class="headerVideo embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item" src="img/source_success.mp4" allowfullscreen></iframe>
      </div>
      </div>
      <div class='col-xs-4'>
<!--                    <iframe width="560" height="315" src="img/3-link-balancing.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen> </iframe>-->
      <div class="headerVideo embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item" src="img/source_fail.mp4" allowfullscreen></iframe>
      </div>
      </div>
            <div class='col-xs-4'>
<!--                    <iframe width="560" height="315" src="img/3-link-balancing.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen> </iframe>-->
            <div class="headerVideo embed-responsive embed-responsive-16by9">
                <iframe class="embed-responsive-item" src="img/target_suc.mp4" allowfullscreen></iframe>
            </div>
            </div>
                </div>


    <div style="margin-top: 40px" class='row'>
      <div style="margin-top: 20px"class='col-xs-3'>
      We see above that our algorithm achieves successful adaptation. However, since we are adapting in an environment with which interation
      could be expensive, we need to adapt as efficiently as possible. The learning curves, a comparison with other state-of-the-art
      policy adaptation methods, verifies the efficiency of our method. Our method can finish the adaptation as fast as within 10 episodes in the
      unknown environment.
      </div>
      <div class='col-xs-9'>
        <img style="width: 100%" src='img/results.png'>
      </div>
    </div>
</div>

<div class="sechighlight">
<div class="container sec" style="font-size:18px">
  <div class="row">
      <h2>Bibtex</h2>
<pre style="font-size:11px; background-color: #E5E3DD">
  @inproceedings{
      title={Provably Efficient Model-based Policy Adaptation},
      author={Yuda Song, Aditi Mavalankar, Wen Sun, Sicun Gao},
      booktitle={ICML},
      year={2020}
  }
</pre>
    </div>
  </div>
</div>
</div>


</body></html>
