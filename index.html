<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-168693378-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-168693378-1');
  </script>

  <title>Yuda Song</title>

  <meta name="author" content="Yuda Song">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuda Song</name>
              </p>
              <p>Hi, I am a second year PhD student at <a href="https://www.ml.cmu.edu/"> Machine Learning Department,
                Carnegie Mellon University</a>. I am coadvise by <a href="http://www.cs.cmu.edu/~aarti"> Aarti Singh</a> and 
                <a href="http://robotwhisperer.org/"> Drew Bagnell</a>. 
                I am also closely working with <a href="http://wensun.github.io">Wen Sun</a>.
                I finished my master degree in MLD, 
                advised by <a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>. 
                I completed my undergraduate at UC San Diego with CS and Math double major and
                I was advised by <a href="http://scungao.github.io">Sicun Gao</a>.

              <p> My email is yudas at andrew dot cmu dot edu.
                
              </p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=0QDCG8IAAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp/&nbsp
                <a href="CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/yudasong">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/yus167">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="DSCF1330.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="DSCF1330.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                 I am broadly interested in machine learning, especially reinforcement learning.
                 I am current focusing on provably statistically and computationally efficient settings and algorithms in RL. 
            </td>
          </tr>
        
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Preprints</heading>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="img/reptransfer.png" width="100%"></td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2205.14571">Provable Benefits of Representational Transfer in Reinforcement Learning </a>
              <br>
              (alphabetical order) Alekh Agarwal, <strong>Yuda Song</strong>, Wen Sun, Kaiwen Wang, Mengdi Wang, Xuezhou Zhang
              <br><br>
                [<a href="https://github.com/yudasong/briee/tree/reptransfer">code</a>]
              </td>
            </td>
          </tr> 

        </tbody></table>
            </td>
          </tr>
        </tbody></table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Preprint</heading>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2406.01462"><b>Understanding Preference Fine-Tuning Through the Lens of Coverage </b> </a>
              <br>
              <strong>Yuda Song</strong>, Gokul Swamy, Aarti Singh, J. Andrew Bagnell, Wen Sun
              <br>
              2024.
              <br><br>
              We prove that offline contrastive-based method (e.g., DPO)
              requires a stronger coverage property than online RL-based method (e.g., RLHF). We propose 
              Hybrid Preference Optimization to combine the benefits of both offline and online methods.
            </td>
            </td>
          </tr>
        

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="75%" valign="center">
              <a href="http://arxiv.org/abs/2406.07253"><b>Hybrid Reinforcement Learning from Offline Observation Alone</b> </a>
              <br>
              <strong>Yuda Song</strong>, J. Andrew Bagnell, Aarti Singh
              <br>
              <em>ICML</em>, 2024.
            </td>
            </td>
          </tr>

          <tr>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2405.19269"><b>Rich-Observation Reinforcement Learning with Continuous Latent Dynamics</b> </a>
              <br>
              <strong>Yuda Song</strong>, Lili Wu, Dylan J. Foster, Akshay Krishnamurthy
              <br>
              <em>ICML</em>, 2024.
              <br><br>
              We introduce a new theoretical framework, RichCLD (Rich-Observation RL with Continuous Latent Dynamics), in which the agent performs control based on high-dimensional observations, but the environment is governed by low-dimensional latent states and Lipschitz continuous dynamics.
            </td>
            </td>
          </tr>

          <tr>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2205.14571"><b>Provable Benefits of Representational Transfer in Reinforcement Learning</b> </a>
              <br>
              (alphabetical order) Alekh Agarwal, <strong>Yuda Song</strong>, Wen Sun, Kaiwen Wang, Mengdi Wang, Xuezhou Zhang
              <br>
              <em>COLT</em>, 2023.
              <br>
                [<a href="https://github.com/yudasong/briee/tree/reptransfer">code</a>]
              <br><br>
              We prove the benefit of representation learning on diverse source environments which allows efficient learning on the 
              source environment with the learned representation under the low-rank MDPs setting.
            </td>
            </td>
          </tr>

          <tr>
            <!--<td style="padding:20px;width:25%;vertical-align:middle"><img src="img/hyq.png" width="100%"></td>-->
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2303.00694"><b>The Virtues of Laziness in Model-based RL: A Unified Objective and Algorithms</b> </a>
              <br>
              Anirudh Vemula, <strong>Yuda Song</strong>,  Aarti Singh, J. Andrew Bagnell, Sanjiban Choudhury
              <br>
              <em>ICML</em>, 2023.
              <br>
                [<a href="https://github.com/vvanirudh/LAMPS-MBRL">code</a>]
              </td>        
            </td>
          </tr>
          
          <tr>
            <!--<td style="padding:20px;width:25%;vertical-align:middle"><img src="img/hyq.png" width="100%"></td>-->
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2210.06718"><b>Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient</b> </a>
              <br>
              <strong>Yuda Song</strong>*, Yifei Zhou*, Ayush Sekhari, J. Andrew Bagnell, Akshay Krishnamurthy, Wen Sun
              <br>
              <em>ICLR</em>, 2023.
              <br>
                [<a href="https://github.com/yudasong/HyQ">code</a>]
              <br><br>
              Combining online data and offline data can solve RL with both statistical and computation efficiency. 
              Experiments on Montezuma's Revenge reveals that hybrid RL works much better 
              than pure online RL and pure offline RL.
              </td>        
            </td>
          </tr>

          <tr>
            <!--<td style="padding:20px;width:25%;vertical-align:middle"></td>-->
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2210.16976"><b>Representation Learning for General-sum Low-rank Markov Games</b></a>
              <br>
              Chengzhuo Ni, <strong>Yuda Song</strong>, Xuezhou Zhang, Chi Jin, Mengdi Wang
              <br>
              <em>ICLR</em>, 2023.
              <br>
              </td>
            </td>
          </tr>

          <tr>
            <!--<td style="padding:20px;width:25%;vertical-align:middle"><img src="img/comblock.png" width="100%"></td>-->
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2202.00063"> <b> Efficient Reinforcement Learning in Block MDPs: A Model-free Representation Learning Approach</b></a>
              <br>
              Xuezhou Zhang, <strong>Yuda Song</strong>, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, Wen Sun
              <br>
              <em>ICML</em>, 2022.
              <br>
                [<a href="https://github.com/yudasong/briee">code</a>]
                [<a href="https://youtu.be/KQ0ZtK392yw">Talk at RL theory seminars</a>]
              <br><br>
              An efficient rich-observation RL algorithm that learns to decode from rich observations to latent states 
              (via adversarial training), while balancing exploration and exploitation.
              </td>
            </td>
          </tr>
          
          <tr>
            <!--td style="padding:20px;width:25%;vertical-align:middle"></td> -->
            <td width="75%" valign="center">
              <a href="https://yudasong.github.io/papers/personalized.pdf"><b>No-regret Model-Based Meta RL for Personalized Navigation</b> </a>
              <br>
              <strong>Yuda Song</strong>, Ye Yuan, Wen Sun, Kris Kitani
              <br>
              <em>L4DC</em>, 2022.
              <br>
            </td>
          </tr>


          <tr>
            <!--<td style="padding:20px;width:25%;vertical-align:middle"><img src="img/T2A.png" width="100%"></td>-->
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2110.03659"><b>Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design</b></a>
              <br>
              Ye Yuan, <strong>Yuda Song</strong>, Zhengyi Luo, Wen Sun, Kris Kitani
              <br>
              <em>ICLR</em>, 2022. <strong style="color:red;">Oral presentation</strong>
              <br>
              [<a href="https://sites.google.com/view/transform2act">Project Page</a>]
            </td>
          </tr>

          <tr>
            <!--<td style="padding:20px;width:25%;vertical-align:middle"><img src="img/PCMLP.png" width="100%"></td>-->
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2107.07410"><b>PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration</b></a>
              <br>
              <strong>Yuda Song</strong>, Wen Sun
              <br>
              <em>ICML</em>, 2021
              <br>
              [<a href="https://github.com/yudasong/PCMLP">code</a>]
              <br><br>
              A simple provably efficient model-based algorithm that achieves competitive performance in both dense reward 
              continuous control tasks and sparse reward control tasks that require efficient exploration.
            </td>
          </tr>
          <tr>
            <!--<td style="padding:20px;width:25%;vertical-align:middle"><img src="img/PADA.png" width="100%"></td>-->
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2006.08051"><b>Provably Efficient Model-based Policy Adaptation</b></a>
              <br>
              <strong>Yuda Song</strong>, Aditi Mavalankar,
               Wen Sun, Sicun Gao
              <br>
              <em>ICML</em>, 2020
              <br>
              [<a href="https://yudasong.github.io/PADA">Project Page</a>]
              [<a href="https://github.com/yudasong/policy_adapt">code</a>]
              <br><br>
              We study Sim-to-Real/policy transfer/policy adaptation under a model-based framework 
              resulting an algorithm that enjoyes strong theoretical guarantees 
              and excellent empirical performance.
            </td>
          </tr>

        <!-- </tbody></table>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching Assistant</heading>
              <p>
                <li>UCSD CSE291: Topics in Search and Optimization (Winter 2020)
                <li>UCSD CSE154: Deep Learning (Fall 2019)
                <li>UCSD CSE150: Introduction to AI: Search and Reasoning (Winter 2019, Spring 2020)
                <li>UCSD CSE30:  Computer Organization and Systems Programming (Spring 2019, Winter 2018)
                <li>UCSD CSE11: Introduction to CS & OOP (Fall 2018)
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Service</heading>
              <p>
                <li>Reviewer: ICML (2021-), NeurIPS (2021-), ICLR (2022-), AAAI (2021-2022).
                <li>Top Reviewer: NeurIPS 2022</li>
              </p>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source code from <a href="https://github.com/jonbarron/jonbarron_website">here</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
